// generated by stacks/gong/go/models/orm_file_per_struct_back_repo.go
package orm

import (
	"database/sql"
	"encoding/json"
	"errors"
	"fmt"
	"io/ioutil"
	"log"
	"os"
	"path/filepath"
	"sort"
	"time"

	"gorm.io/gorm"

	"github.com/tealeg/xlsx/v3"

	"github.com/fullstack-lang/gongsvg/go/db"
	"github.com/fullstack-lang/gongsvg/go/models"
)

// dummy variable to have the import declaration wihthout compile failure (even if no code needing this import is generated)
var dummy_Layer_sql sql.NullBool
var dummy_Layer_time time.Duration
var dummy_Layer_sort sort.Float64Slice

// LayerAPI is the input in POST API
//
// for POST, API, one needs the fields of the model as well as the fields
// from associations ("Has One" and "Has Many") that are generated to
// fullfill the ORM requirements for associations
//
// swagger:model layerAPI
type LayerAPI struct {
	gorm.Model

	models.Layer_WOP

	// encoding of pointers
	// for API, it cannot be embedded
	LayerPointersEncoding LayerPointersEncoding
}

// LayerPointersEncoding encodes pointers to Struct and
// reverse pointers of slice of poitners to Struct
type LayerPointersEncoding struct {
	// insertion for pointer fields encoding declaration

	// field Rects is a slice of pointers to another Struct (optional or 0..1)
	Rects IntSlice `gorm:"type:TEXT"`

	// field Texts is a slice of pointers to another Struct (optional or 0..1)
	Texts IntSlice `gorm:"type:TEXT"`

	// field Circles is a slice of pointers to another Struct (optional or 0..1)
	Circles IntSlice `gorm:"type:TEXT"`

	// field Lines is a slice of pointers to another Struct (optional or 0..1)
	Lines IntSlice `gorm:"type:TEXT"`

	// field Ellipses is a slice of pointers to another Struct (optional or 0..1)
	Ellipses IntSlice `gorm:"type:TEXT"`

	// field Polylines is a slice of pointers to another Struct (optional or 0..1)
	Polylines IntSlice `gorm:"type:TEXT"`

	// field Polygones is a slice of pointers to another Struct (optional or 0..1)
	Polygones IntSlice `gorm:"type:TEXT"`

	// field Paths is a slice of pointers to another Struct (optional or 0..1)
	Paths IntSlice `gorm:"type:TEXT"`

	// field Links is a slice of pointers to another Struct (optional or 0..1)
	Links IntSlice `gorm:"type:TEXT"`

	// field RectLinkLinks is a slice of pointers to another Struct (optional or 0..1)
	RectLinkLinks IntSlice `gorm:"type:TEXT"`
}

// LayerDB describes a layer in the database
//
// It incorporates the GORM ID, basic fields from the model (because they can be serialized),
// the encoded version of pointers
//
// swagger:model layerDB
type LayerDB struct {
	gorm.Model

	// insertion for basic fields declaration

	// Declation for basic field layerDB.Display
	// provide the sql storage for the boolan
	Display_Data sql.NullBool

	// Declation for basic field layerDB.Name
	Name_Data sql.NullString

	// encoding of pointers
	// for GORM serialization, it is necessary to embed to Pointer Encoding declaration
	LayerPointersEncoding
}

// LayerDBs arrays layerDBs
// swagger:response layerDBsResponse
type LayerDBs []LayerDB

// LayerDBResponse provides response
// swagger:response layerDBResponse
type LayerDBResponse struct {
	LayerDB
}

// LayerWOP is a Layer without pointers (WOP is an acronym for "Without Pointers")
// it holds the same basic fields but pointers are encoded into uint
type LayerWOP struct {
	ID int `xlsx:"0"`

	// insertion for WOP basic fields

	Display bool `xlsx:"1"`

	Name string `xlsx:"2"`
	// insertion for WOP pointer fields
}

var Layer_Fields = []string{
	// insertion for WOP basic fields
	"ID",
	"Display",
	"Name",
}

type BackRepoLayerStruct struct {
	// stores LayerDB according to their gorm ID
	Map_LayerDBID_LayerDB map[uint]*LayerDB

	// stores LayerDB ID according to Layer address
	Map_LayerPtr_LayerDBID map[*models.Layer]uint

	// stores Layer according to their gorm ID
	Map_LayerDBID_LayerPtr map[uint]*models.Layer

	db db.DBInterface

	stage *models.StageStruct
}

func (backRepoLayer *BackRepoLayerStruct) GetStage() (stage *models.StageStruct) {
	stage = backRepoLayer.stage
	return
}

func (backRepoLayer *BackRepoLayerStruct) GetDB() db.DBInterface {
	return backRepoLayer.db
}

// GetLayerDBFromLayerPtr is a handy function to access the back repo instance from the stage instance
func (backRepoLayer *BackRepoLayerStruct) GetLayerDBFromLayerPtr(layer *models.Layer) (layerDB *LayerDB) {
	id := backRepoLayer.Map_LayerPtr_LayerDBID[layer]
	layerDB = backRepoLayer.Map_LayerDBID_LayerDB[id]
	return
}

// BackRepoLayer.CommitPhaseOne commits all staged instances of Layer to the BackRepo
// Phase One is the creation of instance in the database if it is not yet done to get the unique ID for each staged instance
func (backRepoLayer *BackRepoLayerStruct) CommitPhaseOne(stage *models.StageStruct) (Error error) {

	for layer := range stage.Layers {
		backRepoLayer.CommitPhaseOneInstance(layer)
	}

	// parse all backRepo instance and checks wether some instance have been unstaged
	// in this case, remove them from the back repo
	for id, layer := range backRepoLayer.Map_LayerDBID_LayerPtr {
		if _, ok := stage.Layers[layer]; !ok {
			backRepoLayer.CommitDeleteInstance(id)
		}
	}

	return
}

// BackRepoLayer.CommitDeleteInstance commits deletion of Layer to the BackRepo
func (backRepoLayer *BackRepoLayerStruct) CommitDeleteInstance(id uint) (Error error) {

	layer := backRepoLayer.Map_LayerDBID_LayerPtr[id]

	// layer is not staged anymore, remove layerDB
	layerDB := backRepoLayer.Map_LayerDBID_LayerDB[id]
	db, _ := backRepoLayer.db.Unscoped()
	_, err := db.Delete(layerDB)
	if err != nil {
		log.Fatal(err)
	}

	// update stores
	delete(backRepoLayer.Map_LayerPtr_LayerDBID, layer)
	delete(backRepoLayer.Map_LayerDBID_LayerPtr, id)
	delete(backRepoLayer.Map_LayerDBID_LayerDB, id)

	return
}

// BackRepoLayer.CommitPhaseOneInstance commits layer staged instances of Layer to the BackRepo
// Phase One is the creation of instance in the database if it is not yet done to get the unique ID for each staged instance
func (backRepoLayer *BackRepoLayerStruct) CommitPhaseOneInstance(layer *models.Layer) (Error error) {

	// check if the layer is not commited yet
	if _, ok := backRepoLayer.Map_LayerPtr_LayerDBID[layer]; ok {
		return
	}

	// initiate layer
	var layerDB LayerDB
	layerDB.CopyBasicFieldsFromLayer(layer)

	_, err := backRepoLayer.db.Create(&layerDB)
	if err != nil {
		log.Fatal(err)
	}

	// update stores
	backRepoLayer.Map_LayerPtr_LayerDBID[layer] = layerDB.ID
	backRepoLayer.Map_LayerDBID_LayerPtr[layerDB.ID] = layer
	backRepoLayer.Map_LayerDBID_LayerDB[layerDB.ID] = &layerDB

	return
}

// BackRepoLayer.CommitPhaseTwo commits all staged instances of Layer to the BackRepo
// Phase Two is the update of instance with the field in the database
func (backRepoLayer *BackRepoLayerStruct) CommitPhaseTwo(backRepo *BackRepoStruct) (Error error) {

	for idx, layer := range backRepoLayer.Map_LayerDBID_LayerPtr {
		backRepoLayer.CommitPhaseTwoInstance(backRepo, idx, layer)
	}

	return
}

// BackRepoLayer.CommitPhaseTwoInstance commits {{structname }} of models.Layer to the BackRepo
// Phase Two is the update of instance with the field in the database
func (backRepoLayer *BackRepoLayerStruct) CommitPhaseTwoInstance(backRepo *BackRepoStruct, idx uint, layer *models.Layer) (Error error) {

	// fetch matching layerDB
	if layerDB, ok := backRepoLayer.Map_LayerDBID_LayerDB[idx]; ok {

		layerDB.CopyBasicFieldsFromLayer(layer)

		// insertion point for translating pointers encodings into actual pointers
		// 1. reset
		layerDB.LayerPointersEncoding.Rects = make([]int, 0)
		// 2. encode
		for _, rectAssocEnd := range layer.Rects {
			rectAssocEnd_DB :=
				backRepo.BackRepoRect.GetRectDBFromRectPtr(rectAssocEnd)
			
			// the stage might be inconsistant, meaning that the rectAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if rectAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Rects =
				append(layerDB.LayerPointersEncoding.Rects, int(rectAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Texts = make([]int, 0)
		// 2. encode
		for _, textAssocEnd := range layer.Texts {
			textAssocEnd_DB :=
				backRepo.BackRepoText.GetTextDBFromTextPtr(textAssocEnd)
			
			// the stage might be inconsistant, meaning that the textAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if textAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Texts =
				append(layerDB.LayerPointersEncoding.Texts, int(textAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Circles = make([]int, 0)
		// 2. encode
		for _, circleAssocEnd := range layer.Circles {
			circleAssocEnd_DB :=
				backRepo.BackRepoCircle.GetCircleDBFromCirclePtr(circleAssocEnd)
			
			// the stage might be inconsistant, meaning that the circleAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if circleAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Circles =
				append(layerDB.LayerPointersEncoding.Circles, int(circleAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Lines = make([]int, 0)
		// 2. encode
		for _, lineAssocEnd := range layer.Lines {
			lineAssocEnd_DB :=
				backRepo.BackRepoLine.GetLineDBFromLinePtr(lineAssocEnd)
			
			// the stage might be inconsistant, meaning that the lineAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if lineAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Lines =
				append(layerDB.LayerPointersEncoding.Lines, int(lineAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Ellipses = make([]int, 0)
		// 2. encode
		for _, ellipseAssocEnd := range layer.Ellipses {
			ellipseAssocEnd_DB :=
				backRepo.BackRepoEllipse.GetEllipseDBFromEllipsePtr(ellipseAssocEnd)
			
			// the stage might be inconsistant, meaning that the ellipseAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if ellipseAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Ellipses =
				append(layerDB.LayerPointersEncoding.Ellipses, int(ellipseAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Polylines = make([]int, 0)
		// 2. encode
		for _, polylineAssocEnd := range layer.Polylines {
			polylineAssocEnd_DB :=
				backRepo.BackRepoPolyline.GetPolylineDBFromPolylinePtr(polylineAssocEnd)
			
			// the stage might be inconsistant, meaning that the polylineAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if polylineAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Polylines =
				append(layerDB.LayerPointersEncoding.Polylines, int(polylineAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Polygones = make([]int, 0)
		// 2. encode
		for _, polygoneAssocEnd := range layer.Polygones {
			polygoneAssocEnd_DB :=
				backRepo.BackRepoPolygone.GetPolygoneDBFromPolygonePtr(polygoneAssocEnd)
			
			// the stage might be inconsistant, meaning that the polygoneAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if polygoneAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Polygones =
				append(layerDB.LayerPointersEncoding.Polygones, int(polygoneAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Paths = make([]int, 0)
		// 2. encode
		for _, pathAssocEnd := range layer.Paths {
			pathAssocEnd_DB :=
				backRepo.BackRepoPath.GetPathDBFromPathPtr(pathAssocEnd)
			
			// the stage might be inconsistant, meaning that the pathAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if pathAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Paths =
				append(layerDB.LayerPointersEncoding.Paths, int(pathAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.Links = make([]int, 0)
		// 2. encode
		for _, linkAssocEnd := range layer.Links {
			linkAssocEnd_DB :=
				backRepo.BackRepoLink.GetLinkDBFromLinkPtr(linkAssocEnd)
			
			// the stage might be inconsistant, meaning that the linkAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if linkAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.Links =
				append(layerDB.LayerPointersEncoding.Links, int(linkAssocEnd_DB.ID))
		}

		// 1. reset
		layerDB.LayerPointersEncoding.RectLinkLinks = make([]int, 0)
		// 2. encode
		for _, rectlinklinkAssocEnd := range layer.RectLinkLinks {
			rectlinklinkAssocEnd_DB :=
				backRepo.BackRepoRectLinkLink.GetRectLinkLinkDBFromRectLinkLinkPtr(rectlinklinkAssocEnd)
			
			// the stage might be inconsistant, meaning that the rectlinklinkAssocEnd_DB might
			// be missing from the stage. In this case, the commit operation is robust
			// An alternative would be to crash here to reveal the missing element.
			if rectlinklinkAssocEnd_DB == nil {
				continue
			}
			
			layerDB.LayerPointersEncoding.RectLinkLinks =
				append(layerDB.LayerPointersEncoding.RectLinkLinks, int(rectlinklinkAssocEnd_DB.ID))
		}

		_, err := backRepoLayer.db.Save(layerDB)
		if err != nil {
			log.Fatal(err)
		}

	} else {
		err := errors.New(
			fmt.Sprintf("Unkown Layer intance %s", layer.Name))
		return err
	}

	return
}

// BackRepoLayer.CheckoutPhaseOne Checkouts all BackRepo instances to the Stage
//
// Phase One will result in having instances on the stage aligned with the back repo
// pointers are not initialized yet (this is for phase two)
func (backRepoLayer *BackRepoLayerStruct) CheckoutPhaseOne() (Error error) {

	layerDBArray := make([]LayerDB, 0)
	_, err := backRepoLayer.db.Find(&layerDBArray)
	if err != nil {
		return err
	}

	// list of instances to be removed
	// start from the initial map on the stage and remove instances that have been checked out
	layerInstancesToBeRemovedFromTheStage := make(map[*models.Layer]any)
	for key, value := range backRepoLayer.stage.Layers {
		layerInstancesToBeRemovedFromTheStage[key] = value
	}

	// copy orm objects to the the map
	for _, layerDB := range layerDBArray {
		backRepoLayer.CheckoutPhaseOneInstance(&layerDB)

		// do not remove this instance from the stage, therefore
		// remove instance from the list of instances to be be removed from the stage
		layer, ok := backRepoLayer.Map_LayerDBID_LayerPtr[layerDB.ID]
		if ok {
			delete(layerInstancesToBeRemovedFromTheStage, layer)
		}
	}

	// remove from stage and back repo's 3 maps all layers that are not in the checkout
	for layer := range layerInstancesToBeRemovedFromTheStage {
		layer.Unstage(backRepoLayer.GetStage())

		// remove instance from the back repo 3 maps
		layerID := backRepoLayer.Map_LayerPtr_LayerDBID[layer]
		delete(backRepoLayer.Map_LayerPtr_LayerDBID, layer)
		delete(backRepoLayer.Map_LayerDBID_LayerDB, layerID)
		delete(backRepoLayer.Map_LayerDBID_LayerPtr, layerID)
	}

	return
}

// CheckoutPhaseOneInstance takes a layerDB that has been found in the DB, updates the backRepo and stages the
// models version of the layerDB
func (backRepoLayer *BackRepoLayerStruct) CheckoutPhaseOneInstance(layerDB *LayerDB) (Error error) {

	layer, ok := backRepoLayer.Map_LayerDBID_LayerPtr[layerDB.ID]
	if !ok {
		layer = new(models.Layer)

		backRepoLayer.Map_LayerDBID_LayerPtr[layerDB.ID] = layer
		backRepoLayer.Map_LayerPtr_LayerDBID[layer] = layerDB.ID

		// append model store with the new element
		layer.Name = layerDB.Name_Data.String
		layer.Stage(backRepoLayer.GetStage())
	}
	layerDB.CopyBasicFieldsToLayer(layer)

	// in some cases, the instance might have been unstaged. It is necessary to stage it again
	layer.Stage(backRepoLayer.GetStage())

	// preserve pointer to layerDB. Otherwise, pointer will is recycled and the map of pointers
	// Map_LayerDBID_LayerDB)[layerDB hold variable pointers
	layerDB_Data := *layerDB
	preservedPtrToLayer := &layerDB_Data
	backRepoLayer.Map_LayerDBID_LayerDB[layerDB.ID] = preservedPtrToLayer

	return
}

// BackRepoLayer.CheckoutPhaseTwo Checkouts all staged instances of Layer to the BackRepo
// Phase Two is the update of instance with the field in the database
func (backRepoLayer *BackRepoLayerStruct) CheckoutPhaseTwo(backRepo *BackRepoStruct) (Error error) {

	// parse all DB instance and update all pointer fields of the translated models instance
	for _, layerDB := range backRepoLayer.Map_LayerDBID_LayerDB {
		backRepoLayer.CheckoutPhaseTwoInstance(backRepo, layerDB)
	}
	return
}

// BackRepoLayer.CheckoutPhaseTwoInstance Checkouts staged instances of Layer to the BackRepo
// Phase Two is the update of instance with the field in the database
func (backRepoLayer *BackRepoLayerStruct) CheckoutPhaseTwoInstance(backRepo *BackRepoStruct, layerDB *LayerDB) (Error error) {

	layer := backRepoLayer.Map_LayerDBID_LayerPtr[layerDB.ID]

	layerDB.DecodePointers(backRepo, layer)

	return
}

func (layerDB *LayerDB) DecodePointers(backRepo *BackRepoStruct, layer *models.Layer) {

	// insertion point for checkout of pointer encoding
	// This loop redeem layer.Rects in the stage from the encode in the back repo
	// It parses all RectDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Rects = layer.Rects[:0]
	for _, _Rectid := range layerDB.LayerPointersEncoding.Rects {
		layer.Rects = append(layer.Rects, backRepo.BackRepoRect.Map_RectDBID_RectPtr[uint(_Rectid)])
	}

	// This loop redeem layer.Texts in the stage from the encode in the back repo
	// It parses all TextDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Texts = layer.Texts[:0]
	for _, _Textid := range layerDB.LayerPointersEncoding.Texts {
		layer.Texts = append(layer.Texts, backRepo.BackRepoText.Map_TextDBID_TextPtr[uint(_Textid)])
	}

	// This loop redeem layer.Circles in the stage from the encode in the back repo
	// It parses all CircleDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Circles = layer.Circles[:0]
	for _, _Circleid := range layerDB.LayerPointersEncoding.Circles {
		layer.Circles = append(layer.Circles, backRepo.BackRepoCircle.Map_CircleDBID_CirclePtr[uint(_Circleid)])
	}

	// This loop redeem layer.Lines in the stage from the encode in the back repo
	// It parses all LineDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Lines = layer.Lines[:0]
	for _, _Lineid := range layerDB.LayerPointersEncoding.Lines {
		layer.Lines = append(layer.Lines, backRepo.BackRepoLine.Map_LineDBID_LinePtr[uint(_Lineid)])
	}

	// This loop redeem layer.Ellipses in the stage from the encode in the back repo
	// It parses all EllipseDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Ellipses = layer.Ellipses[:0]
	for _, _Ellipseid := range layerDB.LayerPointersEncoding.Ellipses {
		layer.Ellipses = append(layer.Ellipses, backRepo.BackRepoEllipse.Map_EllipseDBID_EllipsePtr[uint(_Ellipseid)])
	}

	// This loop redeem layer.Polylines in the stage from the encode in the back repo
	// It parses all PolylineDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Polylines = layer.Polylines[:0]
	for _, _Polylineid := range layerDB.LayerPointersEncoding.Polylines {
		layer.Polylines = append(layer.Polylines, backRepo.BackRepoPolyline.Map_PolylineDBID_PolylinePtr[uint(_Polylineid)])
	}

	// This loop redeem layer.Polygones in the stage from the encode in the back repo
	// It parses all PolygoneDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Polygones = layer.Polygones[:0]
	for _, _Polygoneid := range layerDB.LayerPointersEncoding.Polygones {
		layer.Polygones = append(layer.Polygones, backRepo.BackRepoPolygone.Map_PolygoneDBID_PolygonePtr[uint(_Polygoneid)])
	}

	// This loop redeem layer.Paths in the stage from the encode in the back repo
	// It parses all PathDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Paths = layer.Paths[:0]
	for _, _Pathid := range layerDB.LayerPointersEncoding.Paths {
		layer.Paths = append(layer.Paths, backRepo.BackRepoPath.Map_PathDBID_PathPtr[uint(_Pathid)])
	}

	// This loop redeem layer.Links in the stage from the encode in the back repo
	// It parses all LinkDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.Links = layer.Links[:0]
	for _, _Linkid := range layerDB.LayerPointersEncoding.Links {
		layer.Links = append(layer.Links, backRepo.BackRepoLink.Map_LinkDBID_LinkPtr[uint(_Linkid)])
	}

	// This loop redeem layer.RectLinkLinks in the stage from the encode in the back repo
	// It parses all RectLinkLinkDB in the back repo and if the reverse pointer encoding matches the back repo ID
	// it appends the stage instance
	// 1. reset the slice
	layer.RectLinkLinks = layer.RectLinkLinks[:0]
	for _, _RectLinkLinkid := range layerDB.LayerPointersEncoding.RectLinkLinks {
		layer.RectLinkLinks = append(layer.RectLinkLinks, backRepo.BackRepoRectLinkLink.Map_RectLinkLinkDBID_RectLinkLinkPtr[uint(_RectLinkLinkid)])
	}

	return
}

// CommitLayer allows commit of a single layer (if already staged)
func (backRepo *BackRepoStruct) CommitLayer(layer *models.Layer) {
	backRepo.BackRepoLayer.CommitPhaseOneInstance(layer)
	if id, ok := backRepo.BackRepoLayer.Map_LayerPtr_LayerDBID[layer]; ok {
		backRepo.BackRepoLayer.CommitPhaseTwoInstance(backRepo, id, layer)
	}
	backRepo.CommitFromBackNb = backRepo.CommitFromBackNb + 1
}

// CommitLayer allows checkout of a single layer (if already staged and with a BackRepo id)
func (backRepo *BackRepoStruct) CheckoutLayer(layer *models.Layer) {
	// check if the layer is staged
	if _, ok := backRepo.BackRepoLayer.Map_LayerPtr_LayerDBID[layer]; ok {

		if id, ok := backRepo.BackRepoLayer.Map_LayerPtr_LayerDBID[layer]; ok {
			var layerDB LayerDB
			layerDB.ID = id

			if _, err := backRepo.BackRepoLayer.db.First(&layerDB, id); err != nil {
				log.Fatalln("CheckoutLayer : Problem with getting object with id:", id)
			}
			backRepo.BackRepoLayer.CheckoutPhaseOneInstance(&layerDB)
			backRepo.BackRepoLayer.CheckoutPhaseTwoInstance(backRepo, &layerDB)
		}
	}
}

// CopyBasicFieldsFromLayer
func (layerDB *LayerDB) CopyBasicFieldsFromLayer(layer *models.Layer) {
	// insertion point for fields commit

	layerDB.Display_Data.Bool = layer.Display
	layerDB.Display_Data.Valid = true

	layerDB.Name_Data.String = layer.Name
	layerDB.Name_Data.Valid = true
}

// CopyBasicFieldsFromLayer_WOP
func (layerDB *LayerDB) CopyBasicFieldsFromLayer_WOP(layer *models.Layer_WOP) {
	// insertion point for fields commit

	layerDB.Display_Data.Bool = layer.Display
	layerDB.Display_Data.Valid = true

	layerDB.Name_Data.String = layer.Name
	layerDB.Name_Data.Valid = true
}

// CopyBasicFieldsFromLayerWOP
func (layerDB *LayerDB) CopyBasicFieldsFromLayerWOP(layer *LayerWOP) {
	// insertion point for fields commit

	layerDB.Display_Data.Bool = layer.Display
	layerDB.Display_Data.Valid = true

	layerDB.Name_Data.String = layer.Name
	layerDB.Name_Data.Valid = true
}

// CopyBasicFieldsToLayer
func (layerDB *LayerDB) CopyBasicFieldsToLayer(layer *models.Layer) {
	// insertion point for checkout of basic fields (back repo to stage)
	layer.Display = layerDB.Display_Data.Bool
	layer.Name = layerDB.Name_Data.String
}

// CopyBasicFieldsToLayer_WOP
func (layerDB *LayerDB) CopyBasicFieldsToLayer_WOP(layer *models.Layer_WOP) {
	// insertion point for checkout of basic fields (back repo to stage)
	layer.Display = layerDB.Display_Data.Bool
	layer.Name = layerDB.Name_Data.String
}

// CopyBasicFieldsToLayerWOP
func (layerDB *LayerDB) CopyBasicFieldsToLayerWOP(layer *LayerWOP) {
	layer.ID = int(layerDB.ID)
	// insertion point for checkout of basic fields (back repo to stage)
	layer.Display = layerDB.Display_Data.Bool
	layer.Name = layerDB.Name_Data.String
}

// Backup generates a json file from a slice of all LayerDB instances in the backrepo
func (backRepoLayer *BackRepoLayerStruct) Backup(dirPath string) {

	filename := filepath.Join(dirPath, "LayerDB.json")

	// organize the map into an array with increasing IDs, in order to have repoductible
	// backup file
	forBackup := make([]*LayerDB, 0)
	for _, layerDB := range backRepoLayer.Map_LayerDBID_LayerDB {
		forBackup = append(forBackup, layerDB)
	}

	sort.Slice(forBackup[:], func(i, j int) bool {
		return forBackup[i].ID < forBackup[j].ID
	})

	file, err := json.MarshalIndent(forBackup, "", " ")

	if err != nil {
		log.Fatal("Cannot json Layer ", filename, " ", err.Error())
	}

	err = ioutil.WriteFile(filename, file, 0644)
	if err != nil {
		log.Fatal("Cannot write the json Layer file", err.Error())
	}
}

// Backup generates a json file from a slice of all LayerDB instances in the backrepo
func (backRepoLayer *BackRepoLayerStruct) BackupXL(file *xlsx.File) {

	// organize the map into an array with increasing IDs, in order to have repoductible
	// backup file
	forBackup := make([]*LayerDB, 0)
	for _, layerDB := range backRepoLayer.Map_LayerDBID_LayerDB {
		forBackup = append(forBackup, layerDB)
	}

	sort.Slice(forBackup[:], func(i, j int) bool {
		return forBackup[i].ID < forBackup[j].ID
	})

	sh, err := file.AddSheet("Layer")
	if err != nil {
		log.Fatal("Cannot add XL file", err.Error())
	}
	_ = sh

	row := sh.AddRow()
	row.WriteSlice(&Layer_Fields, -1)
	for _, layerDB := range forBackup {

		var layerWOP LayerWOP
		layerDB.CopyBasicFieldsToLayerWOP(&layerWOP)

		row := sh.AddRow()
		row.WriteStruct(&layerWOP, -1)
	}
}

// RestoreXL from the "Layer" sheet all LayerDB instances
func (backRepoLayer *BackRepoLayerStruct) RestoreXLPhaseOne(file *xlsx.File) {

	// resets the map
	BackRepoLayerid_atBckpTime_newID = make(map[uint]uint)

	sh, ok := file.Sheet["Layer"]
	_ = sh
	if !ok {
		log.Fatal(errors.New("sheet not found"))
	}

	// log.Println("Max row is", sh.MaxRow)
	err := sh.ForEachRow(backRepoLayer.rowVisitorLayer)
	if err != nil {
		log.Fatal("Err=", err)
	}
}

func (backRepoLayer *BackRepoLayerStruct) rowVisitorLayer(row *xlsx.Row) error {

	log.Printf("row line %d\n", row.GetCoordinate())
	log.Println(row)

	// skip first line
	if row.GetCoordinate() > 0 {
		var layerWOP LayerWOP
		row.ReadStruct(&layerWOP)

		// add the unmarshalled struct to the stage
		layerDB := new(LayerDB)
		layerDB.CopyBasicFieldsFromLayerWOP(&layerWOP)

		layerDB_ID_atBackupTime := layerDB.ID
		layerDB.ID = 0
		_, err := backRepoLayer.db.Create(layerDB)
		if err != nil {
			log.Fatal(err)
		}
		backRepoLayer.Map_LayerDBID_LayerDB[layerDB.ID] = layerDB
		BackRepoLayerid_atBckpTime_newID[layerDB_ID_atBackupTime] = layerDB.ID
	}
	return nil
}

// RestorePhaseOne read the file "LayerDB.json" in dirPath that stores an array
// of LayerDB and stores it in the database
// the map BackRepoLayerid_atBckpTime_newID is updated accordingly
func (backRepoLayer *BackRepoLayerStruct) RestorePhaseOne(dirPath string) {

	// resets the map
	BackRepoLayerid_atBckpTime_newID = make(map[uint]uint)

	filename := filepath.Join(dirPath, "LayerDB.json")
	jsonFile, err := os.Open(filename)
	// if we os.Open returns an error then handle it
	if err != nil {
		log.Fatal("Cannot restore/open the json Layer file", filename, " ", err.Error())
	}

	// read our opened jsonFile as a byte array.
	byteValue, _ := ioutil.ReadAll(jsonFile)

	var forRestore []*LayerDB

	err = json.Unmarshal(byteValue, &forRestore)

	// fill up Map_LayerDBID_LayerDB
	for _, layerDB := range forRestore {

		layerDB_ID_atBackupTime := layerDB.ID
		layerDB.ID = 0
		_, err := backRepoLayer.db.Create(layerDB)
		if err != nil {
			log.Fatal(err)
		}
		backRepoLayer.Map_LayerDBID_LayerDB[layerDB.ID] = layerDB
		BackRepoLayerid_atBckpTime_newID[layerDB_ID_atBackupTime] = layerDB.ID
	}

	if err != nil {
		log.Fatal("Cannot restore/unmarshall json Layer file", err.Error())
	}
}

// RestorePhaseTwo uses all map BackRepo<Layer>id_atBckpTime_newID
// to compute new index
func (backRepoLayer *BackRepoLayerStruct) RestorePhaseTwo() {

	for _, layerDB := range backRepoLayer.Map_LayerDBID_LayerDB {

		// next line of code is to avert unused variable compilation error
		_ = layerDB

		// insertion point for reindexing pointers encoding
		// update databse with new index encoding
		db, _ := backRepoLayer.db.Model(layerDB)
		_, err := db.Updates(*layerDB)
		if err != nil {
			log.Fatal(err)
		}
	}

}

// BackRepoLayer.ResetReversePointers commits all staged instances of Layer to the BackRepo
// Phase Two is the update of instance with the field in the database
func (backRepoLayer *BackRepoLayerStruct) ResetReversePointers(backRepo *BackRepoStruct) (Error error) {

	for idx, layer := range backRepoLayer.Map_LayerDBID_LayerPtr {
		backRepoLayer.ResetReversePointersInstance(backRepo, idx, layer)
	}

	return
}

func (backRepoLayer *BackRepoLayerStruct) ResetReversePointersInstance(backRepo *BackRepoStruct, idx uint, layer *models.Layer) (Error error) {

	// fetch matching layerDB
	if layerDB, ok := backRepoLayer.Map_LayerDBID_LayerDB[idx]; ok {
		_ = layerDB // to avoid unused variable error if there are no reverse to reset

		// insertion point for reverse pointers reset
		// end of insertion point for reverse pointers reset
	}

	return
}

// this field is used during the restauration process.
// it stores the ID at the backup time and is used for renumbering
var BackRepoLayerid_atBckpTime_newID map[uint]uint
